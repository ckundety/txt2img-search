{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import uuid\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import qdrant_client as qc\n",
    "import qdrant_client.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/ads-dataset/subfolder-0.zip\n",
    "!wget https://storage.googleapis.com/ads-dataset/subfolder-1.zip\n",
    "!unzip subfolder-0.zip\n",
    "!unzip subfolder-1.zip\n",
    "!mkdir -p ./data/ads/images\n",
    "!mv .0/*.jpg ./data/ads/images\n",
    "!mv ./1/*.jpg ./data/ads/images\n",
    "!rmdir ./0\n",
    "!rmdir ./1\n",
    "!rm subfolder-0.zip subfolder-1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_model_name = 'Salesforce/blip-image-captioning-base'\n",
    "embedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioning_pipeline = transformers.pipeline('image-to-text', model=captioning_model_name)\n",
    "embedding_model = transformers.AutoModel.from_pretrained(embedding_model_name)\n",
    "embedding_tokenizer = transformers.AutoTokenizer.from_pretrained(embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_url = 'localhost:6333'\n",
    "qdrant_collection = 'advert_captions'\n",
    "embedding_size = 384\n",
    "qdrant = qc.QdrantClient(qdrant_url)\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=qdrant_collection,\n",
    "    vectors_config=qc.models.VectorParams(size=embedding_size,\n",
    "                                          distance=qc.models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = pathlib.Path(r'.\\data\\ads\\images')\n",
    "np_embeddings_dir = pathlib.Path(r'.\\data\\ads\\minilm_embeddings')\n",
    "\n",
    "np_embeddings_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, batch_size):\n",
    "    import itertools   \n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(itertools.islice(iterator, batch_size)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\qdrantasnmt\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed embeddings for 100 batches\n",
      "completed embeddings for 200 batches\n",
      "completed embeddings for 300 batches\n",
      "completed embeddings for 400 batches\n",
      "completed embeddings for 500 batches\n",
      "completed embeddings for 600 batches\n",
      "completed embeddings for 700 batches\n",
      "completed embeddings for 800 batches\n",
      "completed embeddings for 900 batches\n",
      "completed embeddings for 1000 batches\n",
      "completed embeddings for 1100 batches\n",
      "completed embeddings for 1200 batches\n",
      "completed embeddings for 1300 batches\n",
      "completed embeddings for 1400 batches\n",
      "completed embeddings for 1500 batches\n",
      "completed embeddings for 1600 batches\n",
      "completed embeddings for 1700 batches\n",
      "completed embeddings for 1800 batches\n",
      "completed embeddings for 1900 batches\n",
      "completed embeddings for 2000 batches\n",
      "completed embeddings for 2100 batches\n",
      "completed embeddings for 2200 batches\n",
      "completed embeddings for 2300 batches\n",
      "completed embeddings for 2400 batches\n",
      "completed embeddings for 2500 batches\n",
      "completed embeddings for 2600 batches\n",
      "completed embeddings for 2700 batches\n"
     ]
    }
   ],
   "source": [
    "img_paths = img_dir.glob('*.jpg')\n",
    "\n",
    "total_batches = 0\n",
    "for img_paths_batch in batched(img_paths, batch_size=4):\n",
    "    imgs_batch = [Image.open(img_path).convert('RGB') for img_path in img_paths_batch]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gen_captions_batch = captioning_pipeline(imgs_batch)\n",
    "        captions_batch = [c[0]['generated_text'] for c in gen_captions_batch]\n",
    "        caption_tokens_batch = embedding_tokenizer(captions_batch, padding=True, truncation=True, return_tensors='pt')\n",
    "        embeddings_batch = embedding_model(**caption_tokens_batch)\n",
    "        embeddings_batch = mean_pooling(embeddings_batch, caption_tokens_batch['attention_mask'])\n",
    "        embeddings_batch = torch.nn.functional.normalize(embeddings_batch, p=2, dim=1)\n",
    "\n",
    "    qdpoints = []\n",
    "    for img_path, caption, embedding in zip(img_paths_batch, captions_batch, embeddings_batch):\n",
    "        embedding_np = embedding.cpu().detach().numpy()\n",
    "        qdpoint = qc.models.PointStruct(id=str(uuid.uuid1()),\n",
    "                                        vector=embedding_np,\n",
    "                                        payload={'image': img_path.name,\n",
    "                                                 'location': img_path,\n",
    "                                                 'caption': caption,\n",
    "                                                 'created': str(dt.datetime.now())}\n",
    "        )\n",
    "        np.save(np_embeddings_dir / f'{img_path.stem}.npy', embedding_np)\n",
    "        qdpoints.append(qdpoint)\n",
    "\n",
    "    qdrant.upsert(\n",
    "        collection_name=qdrant_collection,\n",
    "        points=qdpoints\n",
    "    )\n",
    "\n",
    "    total_batches += 1\n",
    "    if total_batches % 100 == 0:\n",
    "        print('completed embeddings for', total_batches, 'batches')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdrantasnmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
